# Chapter 2 Metamathematics

Metamathematics, as the name suggests, goes beyond the derivation of new results from existing axioms and theorems, and focuses instead on the *structure* of proofs. To understand why this sort of analysis is important, we must first say a few words about axioms and the way in which they are chosen. The ancient Greeks believed that axioms ought to be self-evident statements which are consistent with our everyday experience, and can therefore be accepted without proof. This conviction lies at the heart of Euclidean geometry, which grew out of five seemingly obvious truths about lines, angles and circles.

For many centuries, this discipline was viewed as the cornerstone of mathematics, whose logical foundations were beyond doubt. There was, however, a nagging problem with Euclid's fifth axiom, which continued to trouble generations of theoreticians. This axiom (which is known as the Parallel Postulate) an be paraphrased as follows:

__PROPOSITION 5__. Consider $$a$$ line $$l$$ and a point $$a$$ that does not belong to it. Then, there is one and only one line that passes through $$a$$ and does not intersect $$l$$.

The difficulty with the Parallel Postulate stems from the fact that it deals with lines of infinite length. The mathematicians of ancient Greece (and those who followed them in subsequent centuries) could not accept such a statement as a “self-evident” truth, since they believed that human intuition is limited to finite objects. As a result, the two millennia that followed Euclid's death witnessed countless attempts to close the loophole, and derive Proposition 5 on the other four axioms. The status of this proposition was finally resolved the 19th century, when mathematicians Nikolai Lobachevsky and János Bolyai established that such a derivation is impossible.

The recognition that Proposition 5 was *unprovable* in the framework of classical geometry formally justified Euclid's decision to incorporate it into his system an additional axiom. However, it also opened up the possibility that the opposite of this proposition could be an axiom in its own right (provided, of course, that the resulting system remains free of contradictions). This realization led the development of several non-Euclidean geometries, whose results are anything but “self-evident.” Although such geometries seem to defy our intuitive understanding of spatial relationships, it would be a mistake to dismiss them. “artificial” constructs of the human mind. Einstein's theory of general relativity provides an excellent illustration of this point, since it uses non-Euclidean geometry to describe the structure of space and time (both of which are fundamental constituents of physical reality).
 
One of the most important insights that can be gained from the results of Lobachevsky and Bolyai is that axioms can actually be completely divorced from human experience. Nineteenth century mathematicians quickly capitalized on this possibility, and began to think of axioms as strings of completely meaningless symbols which can be manipulated according to a set of precisely defined rules. This conceptual shift marked the birth of a new paradigm, whose central claim was that all mathematical thinking could be formally described in such abstract terms.

Throughout the early decades of the 20th century, it was widely believed that this approach would eventually resolve all remaining ambiguities, and that the Platonic ideal of a “perfect” mathematical theory was finally within our reach. This seemingly unstoppable tide of optimism was suddenly reversed 1931, when Austrian mathematician Kurt Gödel published his famous Incompleteness Theorem. Gödel showed that any sufficiently complex formal system must necessarily contain unprovable propositions, and that no amount of additional axioms can resolve the problem. He established, in other words, that mathematics is *inherently incomplete*, and that certain truths will remain unknowable to us. This was (and continues to be) a very surprising conclusion, since one intuitively expects that it is always possible to establish whether or not a mathematical proposition is true (at least, in principle).

The fact that certain propositions defy this expectation has a number of important consequences. Among other things, it suggests that “automatic” proof verification is impossible, and that formal algorithms can never completely replace qualified experts when it comes to validating theorems. Under such circumstances, there are no guarantees that a consensus will always be achieved or that subtle errors will not be uncovered at some point in the future. We must concede, in other words, that mathematics cannot be separated from the human thought process, and that this discipline is not infallible.

This inherent “imperfection” of mathematics raises an interesting practical question. In the absence of absolute certainty, what criteria must be met before a proposition can be accepted as *true*? Will mathematics remain “pure", and recognize only statements that have been rigorously proved, or will its practititioners eventually adopt the model used in the natural sciences, where hypotheses are accepted as valid when they are shown to be sufficiently probable? The fact that suggestions of this sort (which would have thoroughly shocked mathematicians of centuries past) are now being seriously discussed suggests that we are witnessing a significant change in the way mathematics is practiced.

Another development that has affected the way we think about mathematics is the emergence of various “experimental” techniques, which are based on computer-aided simulation. This method of investigation has gained wide acceptance in recent years, due in large part to its ability to uncover new and unexpected problems. The potential value of this approach is particularly evident in areas such as chaos theory and fractal geometry, neither of which existed (or could have existed) prior to the advent of computers. It goes without saying, of course, that mathematical results anticipated through simulation still require rigorous proof before they can be accepted as more than conjectures. In that spect, mathematics is still the same as it was centuries ago. It is also true, however, that experimental techniques open up a range of previously unanticipated possibilities, and can often provide us with a very accurate idea of what it is that we should attempt to prove. If this methodology continues to attract new adherents (which seems very likely), it will become increasingly inappropriate to view mathematics as a purely abstract and non-empirical discipline.

In thinking about the impact of Gödel's theorem, it is also important to emphasize that this is by no means the only such result in mathematics. Turing's Halting Problem in computer science, the Continuum Hypothesis in set theory and Chaitin’s constant Omega all point to the existence of unprovable propositions in mathematics. The case of Omega (which we will consider in greater detail in [Section 2.4]()) is particularly interesting in this context, since it has been rigorously shown that we can determine only a very limited number of its bits. The remaining bits in its binary expansion elude computation, and no new theory or improvement in computing power will ever allow us to deduce them. Each of these bits can therefore be viewed as an “unknowable truth”, much like the one discovered by Gödel.

In the following sections we will take a closer look at some of the issues that were raised in this brief introduction. We begin with a description of certain important properties of formal systems, such as internal consistency and completeness. This will provide the necessary background for understanding Gödel's theorem, whose proof is outlined in [Section 2.3](). We will then examine how Gödel's work affected other areas of modern mathematics, such as set theory, non-standard analysis and algorithmic information theory.
