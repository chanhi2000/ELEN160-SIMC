# Chapter 3 Quantum Mechanics

Much of what goes on in the world of quantum mechanics is difficult to describe in the language of classical physics. Concepts such as position, velocity or energy, for example, are still applicable, but must be used with great care in order to avoid erroneous interpretations. Although our mathematical models provide us with a great deal of information about the way microscopic particles behave, we must concede that many of the underlying ideas are highly counterintuitive, and are often best described using analogies and metaphors.

One of the most puzzling motions that we will encounter in this chapter the so-called “state of superposition”, which is used to describe unobserved articles. According to the Copenhagen interpretation of quantum mechanics, such particles exist as *collections of potentialities*, which are mutually exclusive our everyday experience. Descriptions of this sort are clearly difficult to grasp, mce they bring into question some of our most basic assumptions about our physical environment. We can easily envision, for example, an electron whose spin is oriented “up” or “down”, but never both at once. And yet, that is precisely what quantum mechanics asks us to do. This paradoxical situation is resolved only after a measurement is made, at which point exactly one of the competing possibilities “materializes”.

The state of Superposition is formally described in terms of a complex-valued wave function $$\psi(x,\:t)$$, whose temporal evolution is governed by Schrödinger's equation. This equation is deterministic, and allows us to compute $$\psi$$ at any point $$x$$ and at any time $$t$$ given a set of initial conditions. The problem, however, that the wave function provides only partial information about the particle. Indeed, all that we can deduce from Schrödinger's equation are the *probabilities* of different outcomes, and we have no way of knowing what value will actually be recorded. This is not simply a reflection of our ignorance and inability to measure physical quantities at the microscopic level—it is an *irreducible uncertainty*, which is built into the very foundations of quantum mechanics. An obvious consequence of this property is that certain aspects of nature will necessarily main unknowable to us.

The process by which a particle transitions from a state of superposition to a physically observable state poses a further challenge to our conventional understanding of “reality”. This remains one of the most controversial issue in quantum mechanics, and is known as the “measurement problem.” A key concept in this debate is the so-called “collapse” of the wave function, whose essence is still not fully understood (although it has been the subject of intense scrutiny for almost a century). Niels Bohr believed, for example, that a measurement device is a purely “classical” entity, while the particle itself conforms to the laws of quantum mechanics. From that perspective, the collapse of the wave function could be seen as a sort of “bridge”, which somehow enables the particle to cross the gap that separates the microscopic domain from the classical one.

A very different response to this question is provided by the so-called “many worlds” interpretation of quantum mechanics, which was proposed by physicist Hugh Everett in the 1950s. On this view, there is no such thing as a “collapse”, and *each* possible state supposedly materializes in a parallel universe. However strange it may sound, scientists take this theory seriously, since many of its predictions are consistent with existing experimental data.

Perhaps the most intriguing aspect of the measurement problem is the role of the observer. Unlike classical physics, which implicitly assumes that there exists an “external” world which can be studied objectively, quantum mechanics maintains that the physical attributes of a particle actually depend *on the way we choose to observe it*. We could decide, for example, to perform an experiment which would prove that an electron has wavelike characteristics (such as interference and spatial distribution). Alternatively, we could set up our experimental apparatus in a way that would clearly demonstrate the electron's corpuscular nature. Although these two results appear to be at odds with each other, modern physics claims that they are both legitimate, and represent equally valid manifestations of the electron’s “physical reality.” An electron, in other words, exhibits *particle-wave duality*, and the “face” that it will reveal to us depends on our method of investigation. The fact that the choice of measurement strategy is a *conscious* human decision further suggests that there may be no “real” physical world “out there” whose existence is completely independent of the observer.

Although we now have indisputable empirical evidence that particle-wave duality does indeed exist in nature, Some aspects of this unusual property remain elusive. One of the most important unresolved questions in this context concerns the nature of the waves that are associated with particles. An explanation put forth by Max Born in the 1920s suggests that these waves represent time varying probabilities, which propagate through spacetime and interact much like ordinary light waves do. Whether these waves are “physically real” or are just a convenient mathematical tool for describing the particle's motion is still an open question, which cannot be settled empirically (since probabilities have no mass or energy).

A very different interpretation of the “wavelike” nature of matter was proposed by Richard Feynman in the 1960s, Feynman suggested that in moving between two points a quantum particle actually traverses *every* possible jectory simultaneously. He then showed that the combined average over these paths produces the same probabilities as the ones calculated using the wave function. It is interesting to note that these two explanations (each of them counterintuitive in its own right) produce identical predictions, despite that fact they appear to contradict each other. As a result, scientists consider them to be equally acceptable.

Heisenberg's Uncertainty Principle places yet another restriction on what we can say about the microscopic world. Its most widely known version concerns the position and momentum of a particle (denoted in the following by $$x$$ and $$p$$, respectively). If a particle happens to be in a state in which *neither* $$x$$ *nor* $$p$$ have a definite value, the probability distributions of these variables are characterized by their “spreads” $$\Delta{p}$$ and $$\Delta{x}$$, respectively. What Heisenberg recognized was that $$\Delta{p}$$ and $$\Delta{x}$$ are not independent, and must satisfy the inequality $$\Delta{x}\Delta{p}\geq\tfrac{\hbar}{2}$$, where $$\hbar$$ is Planck's constant divided by $$2\pi$$.

A common interpretation of this inequality is that we cannot simultaneously measure the position and velocity of a particle. While such a statement is technically correct, it fails to capture the essence of Heisenberg's result. What the Uncertainty Principle really tells us is that the more we know about the position of a particle, the less we can say about its momentum (and vice versa). Consider, for example, the extreme case when $$\Delta{x}$$, is very small and $$\Delta{p}$$ is very large. Under such circumstances, one could legitimately argue that the concept of momentum (which is a classical attribute) cannot be meaningfully associated with the particle, since all values of $$p$$ are equally likely. This is the point where the language of Newtonian physics becomes inadequate (and can even be thoroughly misleading).

As a final point in this introductory section, we should also say a few words about the famous EPR paradox. Einstein was one of many scientists who rected the view that quantum mechanics is inherently uncertain, and that a particle in a state of superposition is just a collection of potentialities. He suggested instead that the microscopic world conforms to a set of *deterministic* laws, whose precise nature has yet to be determined. In the 1930s, Einstein and his iwo graduate students, Podolski and Rosen, produced what they thought was a decisive argument against the Copenhagen interpretation of quantum theory. This argument, which became known as the EPR paradox, was the subject of heated debates for nearly fifty years. It was finally resolved in the early 1980s, when it was conclusively established that Einstein's conjecture was wrong.

The EPR paradox is best described in terms of an experiment in which an atom with spin zero disintegrates into two smaller particles (say A and B) that fly off in different directions. According to quantum mechanics, prior to a measurement the state of each particle ought to be a superposition of “up” and ‘down” spins. This theory also claims, however, that measuring a definite spin for particle A guarantees that we will subsequently measure the *opposite* spin for B, regardless of the distance between them (this is a direct consequence of the fact that the spin of the “parent” atom was zero).

Einstein argued that there is no physical mechanism that would allow information about measurements made on particle A to be “instantaneously transmitted” to particle B, given that no signal can propagate faster than light. Based on this apparent contradiction, he concluded that quantum mechanics is, in fact, an incomplete theory. He proposed an alternative explanation, according to which particles A and B were really subject to “hidden” deterministic laws, and are both in a state of *definite spin* which is decided at their “birth.” Following this where line of reasoning, he concluded that the behavior of such a system can always be described in terms “classical” probabilities.

Although Einstein and his collaborators were not aware of this at the ti there is actually a way to explicitly test the validity of their hypothesis. This test is based on a property of classical probabilities that was discovered by physicist John Bell in the early 1960s. This simple result proved to be crucial for verifying the validity of “hidden variable” theories, since it provided a condition that the particles must satisfy if their behavior is truly classical. The definitive experiment along these lines was conducted by Alain Aspect and his collaborators in 1981. Their measurements conclusively established that Bell's condition was *violated* in this case, which confirmed that quantum phenomena do not lend themselves to strictly deterministic descriptions.

Given such a result, the only consistent explanation of the EPR paradox is that no matter how far apart A and B are, their common “parentage” prevent us from treating them as separate entities. In technical terms, this means that the two particles Shouldn’t be described by separate wave functions;but rather by a *single* one (even if they end up in different galaxies). When particles A is observed, the *joint* wave function collapses, causing particle B to assume a definite spin value as Well. According to this interpretation quantum mechanics must be viewed as a *non-local* theory, and all microscopic phenomena are somehow connected into a non-decomposable whole. Such a conclusion is truly counterintuitive, and has momentous consequences for the way we perceive physical reality. Among other things, it points to the fact that the “whole” can never be completely known by studying isolated parts, which places a significant restriction on what we can know about complex systems. 

In the sections that follow, we will examine some these results more closely, and will introduce the mathematical formalism that is used in this context Although our discussion will be limited to a relatively small number of topic, it should nevertheless provide the reader with a window into a fascinating and mysterious domain, which challenges the “obvious” truths of our everyday experience.